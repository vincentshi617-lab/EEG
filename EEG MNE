import os
import glob
import mne
import torch
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime
import multiprocessing as mp
from multiprocessing import Pool, cpu_count
from functools import partial
import concurrent.futures



PATIENT_IDS = ["150013054"]

BASE_PATH = "/home/sv25/Desktop/eeg_files+annotations"
SESSIONS_TO_PROCESS = ["ses-20", "ses-21","ses-22","ses-23","ses-24","ses-25","ses-26",
                       "ses-27","ses-28","ses-29","ses-30","ses-31","ses-32","ses-33",
                       "ses-34","ses-35","ses-36","ses-37","ses-38","ses-39","ses-40",
                       "ses-100",]


OUTPUT_BASE_DIR = "/home/sv25/Desktop/eeg_epochs_output"



#### setting up the bipolar montage

def create_longitudinal_bipolar_montage(raw):

    bipolar_pairs = [
        ('Fp1', 'F7', 'Fp1-F7'),            # left temporal chain
        ('F7', 'T3', 'F7-T3'),
        ('T3', 'T5', 'T3-T5'),
        ('T5', 'O1', 'T5-O1'),
        ('Fp2', 'F8', 'Fp2-F8'),            # right temporal chain
        ('F8', 'T4', 'F8-T4'),
        ('T4', 'T6', 'T4-T6'),
        ('T6', 'O2', 'T6-O2'),
        ('Fp1', 'F3', 'Fp1-F3'),            # left parasagittal
        ('F3', 'C3', 'F3-C3'),
        ('C3', 'P3', 'C3-P3'),
        ('P3', 'O1', 'P3-O1'),
        ('Fp2', 'F4', 'Fp2-F4'),            # right parasagittal
        ('F4', 'C4', 'F4-C4'),
        ('C4', 'P4', 'C4-P4'),
        ('P4', 'O2', 'P4-O2'),
        ('Fz', 'Cz', 'Fz-Cz'),              # mid-chain
        ('Cz', 'Pz', 'Cz-Pz'),
    ]

    bipolar_data = []
    bipolar_ch_names = []
    sfreq = raw.info['sfreq']

    for ch1, ch2, ch_names in bipolar_pairs:
        if ch1 in raw.ch_names and ch2 in raw.ch_names:
            data1 = raw.get_data(picks=ch1)[0]
            data2 = raw.get_data(picks=ch2)[0]
            bipolar_data.append(data1 - data2)
            bipolar_ch_names.append(ch_names)

    info = mne.create_info(bipolar_ch_names, sfreq, ch_types='eeg')
    bipolar_raw = mne.io.RawArray(np.array(bipolar_data), info)

    if raw.info['meas_date'] is not None:
        bipolar_raw.set_meas_date(raw.info['meas_date'])
    return bipolar_raw


def extract_csv_annotations(csv_path, sfreq):
    """extract annotations from csv file"""
    try:
        df = pd.read_csv(csv_path)
        events = []
        start_recording_stamp = None
        start_recording_time = None
        stop_recording_time = None

        print(f"Searching for recording markers in CSV: {os.path.basename(csv_path)}")

        found_start = False

        for idx, row in df.dropna(subset=['Text', 'Stamp']).iterrows():
            text = str(row['Text']).strip().lower()
            stamp = float(row['Stamp'])

            if 'start recording' in text or 'recording' in text:
                start_recording_stamp = stamp
                start_recording_time = start_recording_stamp / sfreq
                print(f" Found start recording time: stamp={start_recording_stamp}, Time ={start_recording_time:.3f}s")
                found_start = True
                break
            
        if not found_start:
            print(f"no start recording in CSV")
            if 'Stamp' in df.columns:
                min_stamp = df['Stamp'].min()
                start_recording_stamp = min_stamp
                start_recording_time = start_recording_stamp/sfreq
                print(f"using earliest stamp: {start_recording_stamp}-> {start_recording_time:.3f}")

            else:
                start_recording_stamp = 0
                start_recording_time = 0
                print(f"using default 0")



        print(f"\n extract annotations...")
        event_count = 0
        
        for idx, row in df.dropna(subset=['Text', 'Stamp']).iterrows():
            text = str(row['Text']).strip()
            original_text = text
            text_lower = text.lower()
            stamp = float(row['Stamp'])

            if 'start recording' in text_lower or 'stop recording' in text_lower:
                continue
            
            skip_texts = ['montage:', 'review', 'movement',
                          'eye opened', 'eye closed','video recording on,', 'video recording off,'
                          'video recording fault,', 'started analyzer - xlevent / ecg，','started analyzer - data trends，',
                          'video system error', 'started analyzer - persyst，',
                          'analyzer info - persyst,','arouses','chewing','arousal',
                          'patient event clip', 'headbox reconnected','breakout box *****.',
                          'recording analyzer - persyst,', 'off camera,', 'not on camera,',' recording analyzer - data trends',
                          '*****', '3.0 hz', '6.0 hz', '9.0 hz', '12.0 hz', '15.0 hz', '18.0 hz', '21.0 hz', '25.0 hz', '30.0 hz',
                          'eyes open', 'eyes close', 'dad w/pt', 'mvt']
            
            should_skip = False
            for skip in skip_texts:
                if skip in text_lower:
                    should_skip = True
                    break
            
            if should_skip:
                continue
            
            is_event = False
            
            if text.isdigit():
                is_event = True
            
            elif len(text) <= 10:
                non_events = ['start', 'stop', 'on', 'off', 'error', 'fault']
                if not any(non_event in text_lower for non_event in non_events):
                    is_event = True
            
            if is_event:
                duration = 0.0
                if 'Duration' in df.columns and not pd.isna(row.get('Duration')):
                    duration = float(row['Duration'])
                

                corrected_onset = (stamp - start_recording_stamp) / sfreq
                
                print(f" {event_count+1:03d}: '{original_text}' "
                      f"@ Stamp={stamp}, relative time={corrected_onset:.3f}s")
                
                events.append({
                    'onset': corrected_onset,
                    'duration': duration,
                    'description': original_text
                })
                event_count += 1
        
        print(f"\n stats:")
        print(f"  annotations found: {len(events)}")
        
        if events:
            onsets = [e['onset'] for e in events]
            durations = [e['duration'] for e in events]
            descriptions = [e['description'] for e in events]
            
            annotations = mne.Annotations(onsets, durations, descriptions)
            
            print(f"  first annotation: '{events[0]['description']}' @ {events[0]['onset']:.3f}s")
            print(f"  last annotation: '{events[-1]['description']}' @ {events[-1]['onset']:.3f}s")
            print(f" sucessfully created {len(annotations)} annotations")
            
            return annotations, start_recording_stamp, start_recording_time, stop_recording_time
        else:
            print(f"haven't found any event")
            return mne.Annotations([], [], []), start_recording_stamp, start_recording_time, stop_recording_time
            
    except Exception as e:
        print(f"\n CSV process error: {e}")
        import traceback
        traceback.print_exc()
        return mne.Annotations([], [], []), None, None, None





def process_eeg_session(edf_path, csv_path, output_dir, target_sfreq=256):
    """process a single session and resample to target frequency"""

    print(f"\n{'='*60}")


    print(f"Loading EDF file...")
    raw = mne.io.read_raw_edf(edf_path, preload=False)

    actual_sfreq = raw.info['sfreq']
    print(f"Actual sampling frequency: {actual_sfreq} Hz")
    print(f"Target sampling frequency: {target_sfreq} Hz")
    print(f"Data sample points: {len(raw.times)}")
    print(f"Sample length: {raw.times[-1]:.2f} s")

    # check if resampling is needed
    need_resample = abs(actual_sfreq - target_sfreq) > 0.01



    # extract csv file
    if csv_path:
        print(f"Extracting CSV annotations...")
        csv_annotations, start_recording_stamp, start_time, stop_time = extract_csv_annotations(csv_path, actual_sfreq)

        if csv_annotations:
            raw.set_annotations(csv_annotations)
            print(f"loaded{csv_annotations} annotations from CSV")


        # match the eeg-annotations time
        if start_time is not None:
            csv_offset = start_time
        else:
            csv_offset = 0.0

        start_time = start_time - csv_offset if start_time is not None else None
        stop_time = stop_time - csv_offset if stop_time is not None else None
    else:
        csv_annotations = None
        start_time = None
        stop_time = None


    if start_time is None:
        start_time = 0
        print("Using default start_time = 0")
    if stop_time is None:
        end_time = raw.times[-1]
        print(f"Using default end_time = {end_time:.2f}")
    else:
        end_time = min(stop_time, raw.times[-1])

    print(f"Using start_time: {start_time:.3f} seconds")
    print(f"Using end_time: {end_time:.3f} seconds")

    raw_cropped = raw.copy()
    raw_cropped.crop(tmin=start_time, tmax=end_time)

    artifacts_to_drop = ['OSAT', 'PR']
    existing_artifacts = [ch for ch in artifacts_to_drop if ch in raw_cropped.ch_names]
    if existing_artifacts:
        print(f"Dropping artifact channels: {existing_artifacts}")
        raw_cropped.drop_channels(existing_artifacts)

    print(f"Remaining channels: {len(raw_cropped.ch_names)}")


###### setting up high and lo pass filters
    print(f"Applying filters")
    raw_ref = raw_cropped.copy()
    raw_ref.load_data()

    raw_filter = raw_ref.copy()
    raw_filter.filter(l_freq=0.3, h_freq=70)
    raw_filter.notch_filter(freqs=60)

    raw_recons = raw_filter.copy()
    raw_recons = create_longitudinal_bipolar_montage(raw_recons)
    
    if raw_filter.annotations:
        raw_recons.set_annotations(raw_filter.annotations)

    print(f" Bipolar montage applied: {len(raw_recons.ch_names)} channels")
    print(f"Channel names:{raw_recons.ch_names}")
    print(f"Converting annotations to epochs...")


##### making epochs
    print(f"Converting annotations to epochs...")
    skip_words = ['montage:', 'review', 'movement', 'eye opened', 'eye closed']
    all_descriptions = [ann['description'].lower() for ann in raw_recons.annotations]
    selected_annots = [
        ann for ann,desc in zip(raw_recons.annotations, all_descriptions)
        if not any(skip in desc for skip in skip_words)
    ]

    print(f"Found {len(selected_annots)} valid annotations")

    sfreq = raw_recons.info['sfreq']
    events_list = []
    valid_annotations = []

    for ann in selected_annots:
        onset_sec = ann['onset']
        if 8.0 <= onset_sec <= (raw_recons.times[-1] - 8.0):
            sample_point = int(onset_sec * sfreq)
            events_list.append([sample_point, 0, 1])
            valid_annotations.append(ann)

    if not events_list:
        print("    No valid events within data range")
        return False

    events_array = np.array(events_list)
    print(f"    Converted to {len(events_array)} events")


############### create epochs

    epochs = mne.Epochs(raw_recons,
                        events_array,
                        event_id=1,
                        tmin=-8.0,
                        tmax=8.0,
                        event_repeated='merge',
                        baseline=None,
                        preload=True,
                        verbose=False)

    print(f"Created {len(epochs)} epochs at {actual_sfreq} Hz")

    if need_resample:
        print(f"Resampling from {actual_sfreq} Hz to {target_sfreq} Hz...")
        epochs_resampled = epochs.copy().resample(target_sfreq, npad='auto')
        epochs_to_save = epochs_resampled

    else:
        epochs_to_save = epochs
        print(f"No resampling needed (already at {target_sfreq} Hz)")



    expected_length = int(16 * target_sfreq)
    
    print(f"Preparing {len(epochs_to_save)} epochs for batch processing...")
    
    all_epoch_data = []
    all_annotation_descs = []

    for i in range(len(epochs_to_save)):
        single_epoch = epochs_to_save[i]
        
        if i < len(valid_annotations):
            annotation_desc = valid_annotations[i]['description']
        else:
            annotation_desc = f"unknown_{i}"
        
        epoch_data = single_epoch.get_data()
        
        if epoch_data.shape[0] > 0:
            data = epoch_data[0]
            n_channels = 18
            n_times = expected_length
            
            if data.shape[0] != n_channels:
                if data.shape[0] > n_channels:
                    data = data[:n_channels, :]
                else:
                    padding = np.zeros((n_channels - data.shape[0], data.shape[1]))
                    data = np.vstack([data, padding])
            
            if data.shape[1] != n_times:
                if data.shape[1] > n_times:
                    excess = data.shape[1] - n_times
                    start_idx = excess // 2
                    end_idx = start_idx + n_times
                    data = data[:, start_idx:end_idx]
                else:
                    padding_needed = n_times - data.shape[1]
                    pad_left = padding_needed // 2
                    pad_right = padding_needed - pad_left
                    data = np.pad(data, ((0, 0), (pad_left, pad_right)),
                                 mode='constant', constant_values=0)
            
            tensor = torch.FloatTensor(data).unsqueeze(0)  # [1, 18, n_times]
            safe_desc = annotation_desc.replace('/', '_').replace('\\', '_')
            
            all_epoch_data.append(tensor)
            all_annotation_descs.append(safe_desc)
    
    print(f"Prepared {len(all_epoch_data)} epochs in memory")
    

    def save_epoch_batch(batch_indices, batch_data, batch_descs, output_dir, target_sfreq, start_idx=0):
        """multi epochs"""
        local_saved = 0
        for idx, (tensor, desc) in zip(batch_indices, zip(batch_data, batch_descs)):
            pt_filename = f"epoch_{idx + start_idx + 1:05d}_{desc}_{target_sfreq}Hz.pt"
            pt_filepath = os.path.join(output_dir, pt_filename)
            torch.save(tensor, pt_filepath)
            local_saved += 1
            
            
            if local_saved % 50 == 0:
                print(f"  Worker saved {local_saved}/{len(batch_data)} files")
        
        return local_saved
    

    n_epochs = len(all_epoch_data)
    n_workers = min(cpu_count(), 8) 
    chunk_size = max(200, n_epochs // (n_workers * 3))  
    mp.set_start_method('fork', force=True)
    print(f"Using {n_workers} workers, chunk size: {chunk_size}")
    

    batches = []
    for i in range(0, n_epochs, chunk_size):
        end_idx = min(i + chunk_size, n_epochs)
        batch_indices = list(range(i, end_idx))
        batch_data = all_epoch_data[i:end_idx]
        batch_descs = all_annotation_descs[i:end_idx]
        batches.append((batch_indices, batch_data, batch_descs, i))
    

    print(f"Starting parallel save with {len(batches)} batches...")
    
    saved_count = 0
    with Pool(processes=n_workers) as pool:

        save_func = partial(save_epoch_batch, 
                           output_dir=output_dir, 
                           target_sfreq=target_sfreq)
        
        results = []
        for batch_idx, (indices, data, descs, start_i) in enumerate(batches):
            result = pool.apply_async(save_func, (indices, data, descs, start_i))
            results.append((batch_idx, result))
        
        for batch_idx, result in results:
            try:
                batch_saved = result.get(timeout=300)  # 5分钟超时
                saved_count += batch_saved
                print(f"Batch {batch_idx + 1}/{len(batches)}: saved {batch_saved} files")
            except Exception as e:
                print(f"Error in batch {batch_idx}: {e}")
    
    print(f"Saved {saved_count} PT files to {output_dir}")



##### clear RAM

    del raw, raw_cropped, raw_ref, raw_filter, raw_recons, epochs
    if need_resample:
        del epochs_resampled
    del all_epoch_data, all_annotation_descs
    import gc
    gc.collect()
    return True



def find_session_files(base_path, patient_id, session):
    """check edf and csv file for patient id"""
    eeg_folder = os.path.join(base_path, patient_id, session, "eeg")

    if not os.path.exists(eeg_folder):
        print(f"Folder not found: {eeg_folder}")
        return None, None, None

    edf_files = glob.glob(os.path.join(eeg_folder, "*.edf"))


    csv_patterns = [
        os.path.join(eeg_folder, "*annotations*.csv"),
        os.path.join(eeg_folder, "*.csv")
    ]

    csv_files = []
    for pattern in csv_patterns:
        csv_files.extend(glob.glob(pattern))
        if csv_files:
            break

    if not edf_files:
        print(f"No EDF files found in {eeg_folder}")
        return None, None, None

    if not csv_files:
        print(f"No CSV files found in {eeg_folder}, will proceed without annotations")
        return eeg_folder, edf_files[0], None

    print(f"Found: {os.path.basename(edf_files[0])}")
    print(f"Found: {os.path.basename(csv_files[0])}")

    return eeg_folder, edf_files[0], csv_files[0]




def process_single_session(patient_id, session, edf_path, csv_path, output_base_dir):
    """process a single session"""


    patient_output_dir = os.path.join(output_base_dir, f"patient_{patient_id}")
    session_output_dir = os.path.join(patient_output_dir, session)
    
    # testing directory
    os.makedirs(session_output_dir, exist_ok=True)
    print(f"Output directory: {session_output_dir}")


    test_file = os.path.join(session_output_dir, "test_write_txt")
    try: 
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)

    except Exception as e:
        print(f" canmot write to {session_output_dir}:{e}")
        return False

    try:
        # process eeg data
        success = process_eeg_session(edf_path, csv_path, session_output_dir)

        if success:
            print(f"Processing completed successfully")

            save_processing_summary(patient_id, session, session_output_dir, "success")
            return True
        else:
            print(f"Processing failed")
            save_processing_summary(patient_id, session, session_output_dir, "failed")
            return False

    except Exception as e:
        print(f"Unexpected error: {e}")
        save_processing_summary(patient_id, session, session_output_dir, f"error: {str(e)}")
        return False



def save_processing_summary(patient_id, session, output_dir, status):
    """save processing summary"""
    summary_file = os.path.join(output_dir, "processing_summary.txt")
    with open(summary_file, 'w') as f:
        f.write(f"Patient ID: {patient_id}\n")
        f.write(f"Session: {session}\n")
        f.write(f"Processing Status: {status}\n")
        f.write(f"Processing Time: {pd.Timestamp.now()}\n")
        f.write(f"Output Directory: {output_dir}\n")


def batch_process_epochs():
    """Process all the sessions for one patient in batch"""

    print("\n" + "=" * 70)
    print("Starting Batch EEG Processing")
    print("=" * 70)

    total_patients = len(PATIENT_IDS)
    total_sessions = len(SESSIONS_TO_PROCESS)

    print(f"Total patients to process: {total_patients}")
    print(f"Sessions per patient: {total_sessions}")
    print(f"Total sessions: {total_patients * total_sessions}")
    print(f"Output structure: {OUTPUT_BASE_DIR}/patient_XXXX/ses-X/")
    print()


    successful_sessions = 0
    failed_sessions = 0
    skipped_sessions = 0


    for patient_idx, patient_id in enumerate(PATIENT_IDS, 1):
        print(f"\n{'#' * 60}")
        print(f"PATIENT {patient_idx}/{total_patients}: {patient_id}")
        print(f"{'#' * 60}")

        patient_success = 0
        patient_failed = 0

        for session_idx, session in enumerate(SESSIONS_TO_PROCESS, 1):
            print(f"\n [{patient_idx}.{session_idx}] Processing: {session}")

            # search file
            eeg_folder, edf_path, csv_path = find_session_files(
                BASE_PATH, patient_id, session
            )

            if edf_path is None:
                print(f"Skipping {patient_id}/{session} (no EDF file)")
                patient_failed += 1
                skipped_sessions += 1
                continue


##### process this session
            success = process_single_session(
                patient_id=patient_id,
                session=session,
                edf_path=edf_path,
                csv_path=csv_path,
                output_base_dir=OUTPUT_BASE_DIR
            )

            if success:
                patient_success += 1
                successful_sessions += 1
            else:
                patient_failed += 1
                failed_sessions += 1


            import time
            time.sleep(0.5)

        # processed patient result
        print(f"\n  {'=' * 40}")
        print(f" Patient {patient_id} Summary:")
        print(f" Successful: {patient_success}/{total_sessions}")
        print(f" Failed/Skipped: {patient_failed}/{total_sessions}")





################################################# Final Report for the processing


    print(f"\n{'='*70}")
    print("Batch Processing Complete!")
    print(f"{'='*70}")
    print(f"Total Sessions: {total_patients * total_sessions}")
    print(f"Successful: {successful_sessions}")
    print(f"Failed: {failed_sessions}")
    print(f"Skipped: {skipped_sessions}")

############## I hope print this could be helpful for the data structure
    print(f"\nOutput directory structure:")
    print(f"  {OUTPUT_BASE_DIR}/")
    print(f"    ├── patient_XXXX/")
    print(f"    │   ├── ses-1/")
    print(f"    │   ├── ses-2/")
    print(f"    │   └── ses-3/")
    print(f"    └── processing_log.txt")



    # save
    log_path = os.path.join(OUTPUT_BASE_DIR, "processing_log.txt")
    with open(log_path, "w") as f:
        f.write(f"Processing completed at: {pd.Timestamp.now()}\n")
        f.write(f"Patients processed: {PATIENT_IDS}\n")
        f.write(f"Sessions per patient: {SESSIONS_TO_PROCESS}\n")
        f.write(f"Successful sessions: {successful_sessions}\n")
        f.write(f"Failed sessions: {failed_sessions}\n")
        f.write(f"Skipped sessions: {skipped_sessions}\n")
        f.write(f"\nPatient details:\n")


        for patient_id in PATIENT_IDS:
            patient_dir = os.path.join(OUTPUT_BASE_DIR, f"patient_{patient_id}")
            if os.path.exists(patient_dir):
                f.write(f"\nPatient {patient_id}:\n")
                for session in SESSIONS_TO_PROCESS:
                    session_dir = os.path.join(patient_dir, session)
                    if os.path.exists(session_dir):
                        summary_file = os.path.join(session_dir, "processing_summary.txt")
                        if os.path.exists(summary_file):
                            with open(summary_file, 'r') as sf:
                                f.write(f"  {session}: {sf.read()}")

    print(f"\nDetailed log saved to: {log_path}")

    return successful_sessions, failed_sessions, skipped_sessions



if __name__ == "__main__":
    os.makedirs(OUTPUT_BASE_DIR, exist_ok=True)

    successful, failed, skipped = batch_process_epochs()

    print(f"\nProcessing complete. Check the output directory:")
    print(f"  {OUTPUT_BASE_DIR}")

